# BeatSight - Current Status âœ…

## November 12, 2025 â€“ Web Pivot Planning âœ…

- Captured the browser-first pivot and planning artefacts:
   - `docs/web_pivot_notes.md` for strategy alignment, karma incentives, compute overview.
   - `docs/web_mvp_prd.md` defining product requirements, now cross-linked to schema/task docs.
   - `docs/web_backend_architecture.md` and `docs/web_backend_schema.md` outlining service topology and relational model.
   - `docs/web_mvp_task_breakdown.md` translating the PRD into actionable epics/milestones.
   - `docs/web_compute_costs.md` estimating GPU/CPU spend across growth scenarios.
   - `docs/web_ux_flows.md` storyboarding intake, editor, and verifier UX for desktop/mobile web.
- Added `docs/ml_training_runbook.md` to consolidate dataset export â†’ validation â†’ training â†’ promotion steps; linked from pending items below.
- ROADMAP and NEXT_STEPS updated to reflect the web MVP as Phase 2 and to point engineers at the new planning docs.
- Bootstrap backend code skeleton (`backend/`): FastAPI app, SQLAlchemy models, services, and smoke tests aligned to schema.
- Bundled a handcrafted, heuristics-aligned groove at `shared/formats/simple_beat.{wav,bsm}` with fixed hashes/durations so the sample beatmap demonstrates the AI lane layout out of the box.


## November 11, 2025 â€“ Crash Dual-Label Rebaseline âœ…

- `prod_combined_events.jsonl` relabelled with the latest dual-crash mapping (`crash` 42,362 | `crash2` 28,593; duplication rate 0.0). Health output lives at `ai-pipeline/training/reports/health/prod_combined_dataset_health.{json,html}` with all gates green.
- Sampling weights regenerated from the refreshed manifest: `ai-pipeline/training/reports/sampling/prod_combined_weights_20251111.json` (49,967 session groups, 7,391,699 counted events, 13,079 `crash_dual_label` hits, weights clamped to `[0.05, 0.5]`). Baseline mirrored to `ai-pipeline/training/sampling/weights_prod_combined_20251111.json`.
- Crash coverage boost verified: +8,879 `crash_dual_label` assignments versus the November 8 snapshot. Any downstream training configs should point to the updated sampling artifact before the next run.

## November 9, 2025 â€“ Production Manifest Refresh âœ…

- `prod_combined_events.jsonl` rebuilt with Cambridge sessions plus crash dual relabel. Totals land at **3,010,770 events across 3,891 sessions** with duplication rate 0.0. Baseline diff `ai-pipeline/training/reports/health/diffs/prod_combined_manifest_diff_cambridge_20251109.json` shows **+56,451 events** and **+1,033 sessions** versus `prod_combined_events_pre_crashdual_20251107.jsonl`.
- Crash coverage now splits as `crash` 26,421 (âˆ’15,065 vs. baseline) and `crash2` 16,258 (+16,258). The `crash_dual_label` technique count rises to 16,258, matching the dual-labeled Cambridge hits.
- Production post-ingest checklist (`training/tools/post_ingest_checklist.py` with `health_min_counts_prod.json`, `health_required_labels_prod.txt`, `health_required_techniques_prod.txt`) completed cleanly:
  - Health report: `ai-pipeline/training/reports/health/prod_combined_health_20251109.{json,html}` (all gates âœ…, duplication 0.0).
  - Event loader regression captured in `ai-pipeline/training/reports/health/checklist_prod_combined_20251109.log` (`pytest ai-pipeline/tests/test_event_loader.py -q`).
  - Sampling weights: `ai-pipeline/training/reports/sampling/prod_combined_weights.json` (profile `prod_combined`) with 3,891 session groups, 1,087,401 counted events, crash_dual_label hits 4,200, weights clamped to `[0.05, 0.5]`. Artifact mirrored to `ai-pipeline/training/sampling/weights_prod_combined_20251108.json`.
- Diff, health, and sampling artifacts are archived in `ai-pipeline/training/reports/` for retraining handoff and future audits.

### Pending

1. Export the training bundle `ai-pipeline/training/datasets/prod_combined_20251109` (directory not yet present in workspace); slicing audio per manifest is required before training can start. See `docs/ml_training_runbook.md` for the full sequence.
2. Stage GPU capacity and launch the retrain once the dataset lands. Command template (baseline variant):

   ```bash
   PYTHONPATH=ai-pipeline python ai-pipeline/training/train_classifier.py \
         --dataset ai-pipeline/training/datasets/prod_combined_20251109 \
         --epochs 60 \
         --batch-size 48 \
         --device cuda \
         --wandb-project beatsight-classifier \
         --wandb-tags cambridge refresh_20251109
   ```

   Update tags or run names as needed when logging to Weights & Biases.
3. After the run, capture evaluation metrics, compare against the 2025-11-07 baseline, and note results in `CURRENT_STATUS.md`, `NEXT_STEPS.md`, and `docs/ml_training_runbook.md`.

### November 9 Interim Actions (data restore in progress)

- âœ… Ran `build_training_dataset.py --verify-only --limit 1000` with the new multi-root mapping; all sampled events resolved successfully, so only the still-missing Cambridge folders will surface once the full set is checked.
- ğŸ“‹ Prep command to revisit once the Cambridge recovery finishes:

   ```bash
   python ai-pipeline/training/tools/build_training_dataset.py \
      ai-pipeline/training/data/manifests/prod_combined_events.jsonl \
      /tmp/prod_combined_profile_run \
      --audio-root /mnt/c/Users/10ros/OneDrive/Documents/github/BeatSight/data/raw \
      --audio-root-map slakh2100=/mnt/c/Users/10ros/OneDrive/Documents/github/BeatSight/data/raw/slakh2100 \
      --audio-root-map groove_mididataset=/mnt/c/Users/10ros/OneDrive/Documents/github/BeatSight/data/raw/groove_midi \
      --audio-root-map cambridge_multitrack=/mnt/c/Users/10ros/OneDrive/Documents/github/BeatSight/data/raw/cambridge \
      --audio-root-map cambridge_multitrack=/mnt/d/data/raw/cambridge \
      --manifest-total 3010770 \
      --write-workers 8 \
      --force-rich \
      --overwrite
   ```

- ğŸ“‹ Dataset health follow-up (run after the export lands):

   ```bash
   python ai-pipeline/training/tools/dataset_health.py \
      --dataset-root /tmp/prod_combined_profile_run \
      --dataset-metadata /tmp/prod_combined_profile_run/metadata.json \
      --report-json ai-pipeline/training/reports/health/prod_combined_dataset_health.json \
      --report-html ai-pipeline/training/reports/health/prod_combined_dataset_health.html
   ```

- ğŸ§ª Profiling knobs to keep handy once everything is restored:
   - `--write-workers 4`, `8`, or `12` to probe I/O overlap scaling.
   - `--limit 50000 --force-rich` for a quick smoke export before the full run.
   - Optional `--pad-to 0.75` if we experiment with longer context windows.

- ğŸš¨ Cambridge recovery backlog: 200 folders were deleted from `C:` during the earlier cleanup; they are being redownloaded. Until they return, runs will show the corresponding manifest paths as missing.

### Incident Log â€” Cambridge cleanup misstep (Nov 9)

- During the attempt to replace `data/raw/cambridge` with a symlink to the D: archive, the existing 200 Cambridge-only folders on C: were removed. No duplicates existed on D:, so those folders have to be reacquired.
- A `check_cambridge_presence.py` helper now lives in `ai-pipeline/training/tools/`; run it after the redownload to confirm every manifest path resolves before kicking off the exporter.
- Post-restore plan staged in `ai-pipeline/training/tools/post_restore_plan.sh`; execute it to print the command checklist (export â†’ dataset_health â†’ optional profiling) once the data returns.

### November 10 Status Update

- âœ… Re-ran `check_cambridge_presence.py` with corrected roots (`/mnt/c/.../data/raw`, `/mnt/d/data/raw`); all 2,856 Cambridge manifest entries resolve.
- âœ… Full manifest verification (`build_training_dataset.py --verify-only`) succeeded with zero missing audio at scale.
- ğŸš§ Full export in progress: `build_training_dataset.py` targeting `/tmp/prod_combined_profile_run` with Rich dashboard reporting healthy throughput (~1h15 ETA at start).
- ğŸ§° Post-export checklist staged: see `ai-pipeline/training/tools/post_export_commands.sh` for dataset-health invocation and training launch command.
- ğŸ¯ Training plan drafted: `train_classifier.py --dataset /tmp/prod_combined_profile_run --epochs 60 --batch-size 48 --device cuda --wandb-tags prod_combined_20251109 cambridge_refresh`.

## ğŸ‰ ALL ISSUES RESOLVED!

### âœ… What's Working

1. **.NET SDK 8.0.121** - Successfully installed!
2. **Desktop Application** - Building successfully!
3. **Python Environment** - All dependencies installed including:
   - Demucs (AI music separation)
   - PyTorch & torchaudio
   - librosa & scipy
   - FastAPI server
4. **System Dependencies** - All installed:
   - FFmpeg
   - Audio libraries (OpenAL, ALSA, PulseAudio)
   - Graphics libraries (OpenGL/Mesa)

## ğŸ”§ Issues Fixed

### ~~âš ï¸ NuGet Package Restore~~ âœ… FIXED!

### What Was The Problem?
The project referenced `ppy.osu.Framework.Desktop` which was:
1. Not available on standard NuGet.org
2. Hosted on custom feed at `nuget.ppy.sh` (which has DNS issues)
3. Using an outdated version (2024.1009.0)

### How It Was Fixed âœ…
1. Updated to use `ppy.osu.Framework` version `2025.1028.0` (latest, available on NuGet.org)
2. Removed reference to the non-existent `.Desktop` package variant
3. Fixed API compatibility issue with `Color4.Darken()` method
4. Removed missing `Icon.ico` requirement

**The packages ARE on standard NuGet.org, just needed correct naming and versions!**

## ğŸ® You're Ready to Develop!

### Running the Desktop App
```fish
cd ~/github/BeatSight/desktop/BeatSight.Desktop
dotnet run
```

This will launch the BeatSight desktop application with the main menu!

### Development Commands
```fish
# Build the project
cd ~/github/BeatSight
dotnet build

# Run with hot reload (auto-rebuild on changes)
cd desktop/BeatSight.Desktop
dotnet watch run

# Build release version (optimized)
dotnet build --configuration Release
```

## ğŸ“‹ What You Can Do Now

### 1. Run the Desktop App! ğŸ®
```fish
cd ~/github/BeatSight/desktop/BeatSight.Desktop
dotnet run
```

**What's New:**
- âœ¨ Enhanced gameplay with particle effects and approach circles
- ğŸ“Š Full results screen with grades and statistics
- ğŸµ Speed control (0.25x-2.0x) for practice
- âš™ï¸ **NEW!** Comprehensive settings system with visual effects toggles
- ğŸ® **NEW!** Manual vs Auto gameplay modes (scoring toggle)
- ğŸ“ **NEW!** Practice Mode with section looping and metronome
- âŒ¨ï¸ Retry with 'R' key
- âœï¸ Working editor screen with timeline view and audio loading

### 2. Test the AI Pipeline
The Python AI processing is fully functional:

```fish
cd ~/github/BeatSight/ai-pipeline
source venv/bin/activate.fish

# See available commands
python -m pipeline.process --help

# Test with an MP3 file (if you have one)
python -m pipeline.process --input your-song.mp3 --output output.bsm
```

### 3. Train Your Own AI Model ğŸ¤–
**NEW!** We now have ML-based drum classification:

```fish
cd ~/github/BeatSight/ai-pipeline/training

# Collect training data from beatmaps
python collect_training_data.py --extract-beatmap beatmap.bsm audio.mp3

# Check statistics
python collect_training_data.py --stats

# Export dataset
python collect_training_data.py --export dataset

# Train the model
python train_classifier.py --dataset ./dataset --epochs 50
```

### 4. Current Development Focus
Based on your `notes.txt` and completed work:
1. âœ… **Gameplay screen enhancements** - DONE!
2. âœ… **Results screen** - DONE!
3. âœ… **Basic editor** - DONE!
4. âœ… **ML classifier foundation** - DONE!
5. âœ… **Settings system (23 settings)** - DONE!
6. âœ… **Practice mode with looping** - DONE!
7. âœ… **Visual effects toggles** - DONE!
8. âœ… **Real-time microphone input** - DONE!
9. âœ… **FPS counter and performance monitoring** - DONE!
10. âœ… **Live Input Mode screen** - DONE!
11. âœ… **Volume control integration** - DONE!
12. âœ… **Live input scoring** - DONE!
13. âœ… **Note filtering by difficulty** - DONE!
 14. âœ… **Lane View toggle (2D â†” 3D)** - DONE!
 15. ğŸš§ **Next:** Editor waveform display
 16. ğŸš§ **Next:** Background blur shader
 17. ğŸš§ **Next:** Hit lighting and screen shake

### Stage 0 Hygiene (Nov 2, 2025)
- âœ… **Editor playback guard** â€” `EditorScreen` no longer attempts to run a null track, keeping nullable warnings quiet and status text clear when audio is missing.
- âœ… **3D Runway Preview** â€” Gameplay lane renderer now supports a 3D perspective view; toggle via Settings â†’ Gameplay â†’ Lane View.
- âœ… **Live Input quick-check** â€” Added `docs/LIVE_INPUT_REGRESSION_CHECKLIST.md` with a 5â€‘minute smoke test plus full calibration regression steps.
- âœ… **Docs refreshed** â€” `FEATURE_LIST.md`, `ROADMAP.md`, and this status page all reflect the new mic calibration flow and 3D lane option.
- ğŸ”œ **Stage 0.2+ follow-ups** â€” Continue polishing calibration copy and editor warnings as new telemetry arrives.

### 4. Review Project Documentation
```fish
cat ~/github/BeatSight/OVERVIEW.md
cat ~/github/BeatSight/docs/ARCHITECTURE.md
cat ~/github/BeatSight/ROADMAP.md
cat ~/github/BeatSight/notes.txt
```

## ğŸ” Verification Commands

To verify everything is working:

```fish
# Check .NET SDK version
dotnet --version
# Should output: 8.0.121

# Check build status
cd ~/github/BeatSight
dotnet build
# Should output: Build succeeded.

# Check Python environment
cd ~/github/BeatSight/ai-pipeline
source venv/bin/activate.fish
python --version
# Should output: Python 3.12.x

# Check if Demucs model is downloaded
python -c "import demucs.pretrained; print('Demucs ready!')"
```

## ğŸ“ Your Notes and Goals

From `notes.txt`:
- **Project**: BeatSight (formerly drumify) - Visual drum learning via rhythm game
- **Tech Stack**: 
  - Desktop: C# + osu-framework âœ… (blocked by NuGet)
  - AI: Python + Demucs âœ… (working!)
  - Backend: FastAPI âœ… (ready)
- **Current Phase**: Phase 1 - Desktop-only, local processing
- **Next Tasks**: 
  1. Build gameplay screen with falling notes (requires desktop to work)
  2. Implement editor with timeline/waveform
  3. Real-time mic input detection

## ğŸ¯ Immediate Next Steps

1. **Try Option 2** (Alternative DNS) - most likely to work
2. If that doesn't work, **try Option 4** (VPN)
3. Meanwhile, familiarize yourself with the AI pipeline
4. Check back in 30 minutes to see if ppy server is responsive

## ï¿½ Changes Made to Fix Issues

1. **Updated NuGet.config** - Added configuration to use standard NuGet.org
2. **Updated package references**:
   - `BeatSight.Desktop.csproj`: Updated to `ppy.osu.Framework` 2025.1028.0
   - `BeatSight.Game.csproj`: Updated to `ppy.osu.Framework` 2025.1028.0
3. **Fixed code compatibility**:
   - Replaced `colour.Darken(0.5f)` with manual RGBA calculation
4. **Removed missing files**:
   - Removed `ApplicationIcon` reference to `Icon.ico`

## ğŸš€ Quick Start Guide

### First Time Running
```fish
# Navigate to project
cd ~/github/BeatSight

# Run the desktop app
cd desktop/BeatSight.Desktop
dotnet run
```

### Development Workflow
```fish
# Make code changes in VS Code
# The C# Dev Kit extension provides IntelliSense

# Build to check for errors
dotnet build

# Run with hot reload for rapid development
dotnet watch run
```

## ğŸ“ Resources

- **osu-framework**: https://github.com/ppy/osu-framework
- **osu-framework docs**: https://github.com/ppy/osu-framework/wiki
- **Demucs AI**: https://github.com/facebookresearch/demucs
- **Your project docs**: `~/github/BeatSight/docs/`

---

**Current Date**: November 2, 2025  
**Status**: âœ… ALL SYSTEMS GO! ğŸš€  
**Phase**: 1.2 - Gameplay Implementation (âœ… 100% COMPLETE!)  
**Next Action**: Start Phase 1.3 - Polish & Testing! ğŸ®

## ï¿½ PHASE 1.2 COMPLETE! (Nov 2, 2025 - Session 4)

### Final Three Features - 100% Complete! âœ…
- **Volume Control Integration**: Master and music volume now control actual playback
- **Live Input Scoring**: Microphone hits trigger scoring system correctly
- **Note Filtering**: Difficulty slider filters notes in real-time (25%-100%)

### What's Now Fully Functional
- âœ… **Volume Settings**: Adjust Master/Music volume â†’ Hear changes immediately
- âœ… **Live Input Gameplay**: Clap/drum into mic â†’ Get scored like keyboard
- âœ… **Practice Mode**: Adjust difficulty slider â†’ Note density changes instantly
- âœ… **All 4 Gameplay Modes**: Auto, Manual, Practice, Live Input (all working!)
- âœ… **All 23 Settings**: Every setting is functional

### Build Status
```
Build succeeded.
    0 Warning(s)
    0 Error(s)
```

## ï¿½ğŸ¯ Latest Updates (Nov 2, 2025 - Session 3)

### Expanded Settings System (osu! Inspired) âš™ï¸
- **23 Total Settings**: Up from 13, comprehensive customization
- **New Gameplay Settings**: Background dim/blur, hit lighting, hit error meter, screen shake
- **FPS Counter**: Real-time performance monitoring with color-coded display
- **UI Scale**: Adjust interface size (50%-150%)
- **Performance Controls**: Frame limiter with VSync and FPS cap options
- **Audio Enhancements**: Separate hitsound volume and offset controls

### Real-Time Microphone Input ğŸ¤
- **MicrophoneCapture**: NAudio-based audio capture system
- **RealtimeOnsetDetector**: Spectral flux-based drum hit detection
- **LiveInputModeScreen**: Complete gameplay mode using microphone
- **Drum Classification**: Automatic detection of Kick, Snare, Tom, Cymbal, Hi-Hat
- **Calibration Workflow**: Ambient noise + per-drum signature capture, persisted between sessions
- **Visual Feedback**: 7-meter real-time level display
- **Anti-Double-Trigger**: 40-50ms protection against duplicate hits
- **Device Management**: Automatic microphone enumeration and selection

### Implemented Features from osu! ğŸ®
- **Background Dim**: Functional with real-time adjustment (0-100%)
- **FPS Counter**: Top-right display with color coding (green/yellow/red)
- **Settings Architecture**: 5 sections (Gameplay, Visual, Audio, Input, Performance)
- **Frame Limiter**: Enum-based system ready for implementation

### Technical Additions ğŸ”§
- **NAudio 2.2.1**: Cross-platform audio capture library
- **Audio Processing Pipeline**: Microphone â†’ Capture â†’ Onset Detection â†’ Gameplay
- **8-Band Spectrum Analysis**: Simplified FFT for drum type classification
- **Bindable Integration**: All settings reactive with instant UI updates

## ğŸ¯ Latest Updates (Nov 2, 2025 - Session 2)

### Settings & Customization System âš™ï¸
- **Configuration Manager**: INI-based persistent settings (beatsight.ini)
- **Settings Screen**: Professional UI with 4 categories (Gameplay, Visual, Audio, Input)
- **Visual Effect Toggles**: 5 customizable effects (approach circles, particles, glow, burst, milestones)
- **Gameplay Modes**: Auto (scoring) vs Manual (play-along without scoring)
