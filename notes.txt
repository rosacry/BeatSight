BeatSight Project - Development Notes
====================================

PROJECT RENAMED: drumify ‚Üí BeatSight
Reason: "drumify" was already taken. BeatSight captures the visual learning aspect perfectly.

CURRENT STATUS: Foundation Complete ‚úÖ

What's Been Built:
------------------
1. ‚úÖ Complete project structure with all major components
2. ‚úÖ Desktop app skeleton using osu-framework
3. ‚úÖ Full AI processing pipeline with Demucs integration
4. ‚úÖ Beatmap format specification (.bsm files)
5. ‚úÖ Comprehensive documentation (architecture, setup, contributing)
6. ‚úÖ FastAPI server for remote processing
7. ‚úÖ All core data structures and file I/O

Tech Stack Decisions:
---------------------
‚úÖ Desktop: C# + osu-framework (battle-tested for rhythm games)
‚úÖ AI: Python + Demucs + PyTorch (state-of-the-art)
‚úÖ Backend: FastAPI (Python, async, fast)
‚úÖ Mobile: Flutter (future, cross-platform)
‚úÖ Database: PostgreSQL + S3 for audio (future)

Next Immediate Steps:
---------------------
1. Build gameplay screen with falling notes
2. Implement editor with timeline/waveform
3. Add real-time microphone input detection
4. Train ML model for better drum classification
5. Create mobile apps (Flutter)

Key Files to Start With:
------------------------
Desktop App Entry: desktop/BeatSight.Desktop/Program.cs
Main Game Logic: desktop/BeatSight.Game/BeatSightGame.cs
AI Pipeline Entry: ai-pipeline/pipeline/process.py
Beatmap Format: docs/BEATMAP_FORMAT.md
Architecture: docs/ARCHITECTURE.md

Development Commands:
---------------------
# Desktop
cd desktop/BeatSight.Desktop && dotnet run

# AI Pipeline  
cd ai-pipeline
source venv/bin/activate.fish
python -m pipeline.process --input song.mp3 --output beatmap.bsm

# API Server
python -m pipeline.server

Project Philosophy:
-------------------
- Free and open-source forever
- No ads, ever
- Community-driven development
- Learning-focused (both for users and contributors)
- Quality over speed

Patent Strategy:
----------------
- Free + open source = safer legally
- Guitar Hero mechanics are generic now (patents expired)
- Avoid exact visual designs
- Focus on transformative use (education)

Future Features:
----------------
‚úÖ Real-time play-along scoring (planned)
‚úÖ Sample extraction from favorite bands (planned)
‚úÖ Community beatmap sharing (backend ready)
‚úÖ Distributed AI model training (architecture ready)
‚úÖ Approach rate customization (in beatmap format)
‚úÖ BPM adjustment and metronome (in beatmap format)
‚è≥ Multi-instrument support (bass, guitar, etc.)
‚è≥ VR mode for immersive learning

Community Ideas:
----------------
- Weekly beatmap challenges
- Leaderboards for popular songs
- Tutorial series for beginners
- Collaboration features (multi-track sessions)
- Integration with electronic drum kits (MIDI)

Known Challenges:
-----------------
1. Low-latency audio (critical for rhythm games)
   ‚Üí osu-framework handles this well ‚úÖ
   
2. AI accuracy for complex drum patterns
   ‚Üí Start simple, improve incrementally with ML
   
3. Copyright issues with audio files
   ‚Üí Store drum stems only (transformative use) ‚úÖ
   ‚Üí User uploads at own risk (ToS disclaimer)
   
4. Mobile performance vs. visual quality
   ‚Üí Adaptive graphics settings
   ‚Üí Prioritize gameplay smoothness

Testing Plan:
-------------
1. Unit tests for data structures
2. Integration tests for AI pipeline
3. Performance tests for gameplay (60+ FPS)
4. User testing with real drummers
5. Accessibility testing

Deployment Strategy:
--------------------
Phase 1 (MVP): Desktop-only, local processing ‚Üê YOU ARE HERE
Phase 2: Add backend API, community features  
Phase 3: Mobile apps (iOS/Android)
Phase 4: Advanced features (real-time, VR, etc.)

Success Metrics:
----------------
- User engagement (daily active users)
- Beatmap quality (community ratings)
- AI accuracy (precision/recall on test set)
- Community contributions (PRs, beatmaps)
- User testimonials (learning success stories)

Personal Goals:
---------------
- Build a portfolio-worthy project
- Learn game development with osu-framework
- Master audio processing and ML
- Contribute to open-source community
- Help drummers learn more effectively

Original Vision:
----------------
"I became a top osu! player where sight-reading high difficulty songs
was easy. Why not make this same connection with learning drums?
From osu, I use a tablet to aim and a keypad to tap. With drums,
I use drum sticks to aim and tap. This project bridges my love for
rhythm games, drums, and AI to help others avoid the frustration
of learning by endless repetition. Make it fun, make it visual,
make it accessible."

Let's build something amazing! ü•ÅüéÆ‚ú®



here is the additional context (wrapped in quotes) for this prompt to help you generate perfect prompts for copilot for these issues:

original copy and paste text (do not delete):

I would to make a really cool side project with your help. When I was little, I always love listening to music, but the drumming had always caught my attention most in any song. I decided to learn how to play drums, but it came at a cost. As most drummers do, they get lazy to read (or learn how to read) sheet music, so they decide to just keep listening to the song they want to learn. The problem with this is that when they decide to learn how to play drums for a song they like, likely what happens is when they keep replaying the song over and over (even at slower speeds to understand it better), but they end up getting more and more bored of the songs, and frustration builds. A little side story now but I became I top osu! (circle clicking game) player a while ago, where cite reading any high difficulty song was very easy and I could do it with no issue. The interesting thing I find is that why not make this same connection with learning drums? From osu, I use a tablet to aim and a keypad to tab, with drums, I use drum sticks to aim and tap upon hitting it. My idea is to create an AI, that processes instruments (drums first) into a guitar-hero like mapping where the flow of how the notes are shown become in sync with the song audio (or just the songs drums audio). I would like for this to be a free and free of advertisement app on ios, andriod, and on pc (havent decided if the pc version should be a website or a downloadable software). The user should be able to upload any song of their choice, the program isoloates to the instrument of the users choice (drums only for now) and the AI for the program processes it into a visual guitar-hero like mapping (where patents are dodged of course (but i'm not sure if I should dodge any patents since i'm deciding to make this free in the first place? you decide what's best for this course of action) ), in the best and easiest way possible for the user to associate what notes are for what part of the (drumset) instrument (You may also decide what the best way to do this is). Also, I currently have the name of this project as drumify, which I already know is an existing program, but I would like for you to think of a clever name(s) for this.


Notes and specifications on the program (read EVERYTHING BELOW before proceeding):
-It should detect how many drumset parts are in the audio and specifically what parts (like the crash, china, ride, etc).
-There should be a toggle between playing the entire song or just isolating the instrument that they mapped the song to (the users initial choice of what instrument they chose to map the song to) (again, drums for now)
-There should also be a bpm toggle that plays a bpm sound (with a variety of selection) in the background with the drums.
-There should be a slider that slows down the audio however much they like
-If the offchance that the AI has messed up with mapping the song to visual notes, the program should allow the user to edit and modify the notes to fix it.
-There should be an upload feature tool that sends the file of the mapping with the audio (or only instrument specific audio if copyright issues) to the online server for this program for anyone to access (so they don't have to wait for an AI to process their song)
-There should also be a feature where if the user does not decide to use the AI for their song (or even if they're trying to create a song) there should be an editorial mode of this program (similar to the edit thing i mentioned before but i'm being more descriptive about it) where they an edit an existing song (like I mentioned before) or create a song of their choosing from scratch. for this, they also have a choosing of instrument samples. Instrument Samples for this program are extracted from a specific instrument where they can use those sounds to create their own thing. For example, if a user really likes the drums of a specific band, the AI for this program can also extract all of the different parts of the drumset from that bands songs and map it to their respective part of the drumset (like the cymbal, snare, toms, etc. (with the correct labeling (like china, ride, crash)))
-Just like how there's an approach rate in osu (how fast the outer circle closes in on the main circle before the user needs to tap) there should also be like an approach rate option in this program with show how fast the notes slide down from the top (since it's a guitar hero-like visual, unless you have a better idea) 
-This one thing is an experimental idea and I'm not sure how possible it can be implemented, but I always wanted there to be like this real-time feature where the program (if on pc, they would need a microphone, and on mobile the microphone would need to be enabled too) could listen to you playing along to the mapping the AI (or the user) created, and it could detect realtime if they are hitting the right notes and give you a score in the end (like how osu! does it), and during the gameplay it could give you something similar to what osu! does where it give you 100 if you're too late or too early (300 is on time I think but most osu! users keep that off for distraction, I'd like to keep it that way with this program too)
-I would like there to be a donate button for this program when it explains that all donations go straight back into the program itself (like server costs)
-Decide if this is an AI that needs to be built with training or not. If it does need training, then I would like for you to build a separate sector of this project (but still implemented) where peers with high powered computers can join in on the training set for this application at just a click of a button (probably on pc for this project only, not mobile, unless you beg to differ). This is only if you yourself decide that this program is better off being separately trained or not.
-Have the AI able to process multiple different kinds of audio files
- I am using linux (ubuntu), with kitty, fish, and vscode. When it comes to the IOS development for this project, I will switch to a Mac. If you choose to do a software approach for the PC version, I would also like this software to be compatible for Windows, MacOS, and Linux.
-Maybe it could be in your interest to fork/use the osu-framework (from https://github.com/ppy/osu-framework) for this project. 
-I also really want you to analyze the guitar hero layout that that game does, I want you to make it the same with this program/game. One key feature I can think about is how the bass drum are kept as lines across the rift in the back. Think of the other features it has and implement the one's you think will go best with my program.
-I would also like for you to create a calibration-like feature, where if the user has not done it yet, the program would ask the user (only for the mic detection scoring mode of course) to calibrate the mic detection program-thing by asking the user to like.. hit each part of their drum set for the program/(AI?) to recognize what it needs to listen to. If the program needs to go further to really understand what it's hearing (which is up to you to decide that), then it can ask the user to play a certain beat or something? This is entirely up to you to figure out how to go about this since you definitely have more of a knowledge base behind this and everything else.


I want this to be a full fledged application. You are able to choose the best tools for this application yourself. It's up to you to make this program for me, you are welcome to add anything else you'd like that I didn't already mention since I trust you. I want you to try your best on it. Be clever, intuit, and ingenious with this.

Make sure before implementing these things and anything else, I want you to look at the '.md' files and notes.txt (and notes2.txt if need be) so you have all the context you need to approach these things for this project. You're also more than welcome to take a look at the osu! github open-source repository to copy anything over or just reference (from https://github.com/ppy/osu). Time is not a contraint for me, take all the time you need to work on this, and work to the best of your ability






Additional Things:

(part of 2) )I want you to be sure that if the shazam-like-ai tools ends up failing to fetch the title and artist from a song (since the audio file could be either an undocumented song or just a sample piece), if the user tries leaving the editorial screen it should prompt the user to say that either the Title and/or artist wasn't found, to enter in whatever is missing before they relaunch the editorial or play the song (in play mode or practice mode whatever), if they decide not to add the missing/remaining details/metadata about the song, then this program should autocreate it for them (like autoname it to sample title and/or sample artist (or if you can think of a better temp/placeholder name that would be great))

This also means that the audio file dropped into the program, opening it through the editorial (or first the generative AI), does not save unless the user clicks the save button or tries to leave the editorial to go back to the homepage, in which the program will ask if they would like to save the mapping, (where it would then prompt the user to pass a title or artist as well if they wish)


Also you need make sure in the editorial menu that it very very closely to how osu! does it, (for Chatgpt: I've attached the screenshot for you, I want you to go into great detail about all the contents and everything you notice to do in the image, and compare it to my current editorial image, which is the second attached image)




Ontop of the ranked/scoring mode (that has the live microphone detection), there should exist a mode (similar to the flashlight mod in osu!) where the notes only appear the moment you hit it on time. This mode should be like a professional mode, where after the user fully recognizes and memorizes the map (or if the user just knows how to play the entire song perfectly), they would play this mode. Full comboing this mode and hitting everything on time gives the user highest score possible for this song.




My next vision (which looks like it's all around the place):
I would like the website for this project to feature these things. The home page is simple and straightforward, as in, the user can drag and drop any song file, the shazam-like-AI tool will attempt to find the song title and artist (and maybe the bpm), and if not, before the user saves the file, the program would prompt the user to enter in a title and artist (if they wish, if not that it gets auto generated). But after the shazam-like-AI tool finishes, the shazam-like-AI tool will analyze the programs server to check if there's already an existing mapping of the song that the user is looking for (ONLY if the map is verified, there should NEVER contain more than 1 'verified' mapping of a song on the servers database).  This should also be a thing for the downloadable program too, as long as they're connected to the internet (since why not). But the difference is, you can use the AI map generation tool as much as you want on the downloadable program, but you can only use the AI map generation on the website if the mapping of the song doesn't already exist. This also means that if a user decides to use the AI on the downloadable program, even after the program tells the user that the mapping already exists from the servers database (if the user is connected to the internet that is), they cannot upload the song's mapping to the servers database since it already exists there. There should also exist a feature like a karma system for the tool, where registered users can upvote or downvote the current mapping of a song that's stored on the servers data base. From this, the program calculates the likelihood of the mapping being all correct or not from the ratio of upvotes and downvotes of the song (i'm not too sure about this idea/sentence specifically, i'll let you to decide how to change it or anything after you finish reading everything). If a user downvotes, they must point out the fix for the song. The correction is then verified from a user with a good amount of karma (you can decide the baseline). If the fix is correct, then user that fixed it earn a good sum of karma for their account, and user who verified it, earns karma too (but a little less). On both the website version and downloadable version of the program, the user is allowed to make edits to the existing song from the server, the edits are local changes, but upon them saving the edits as well, they are also able to upload the edits of it to the maps index for review if those edits were made with the intent to fix the map (if this makes sense). Quick side note, but I think registered users should have both a verified email and a verified phone number if they want to upvote/downvote. I also think if the user has both a verified email and a verified phone number, the amount of karma you get from each of those combined, should sum to enough to have enough karma to be a verifier for a map, while just a verified email should be just enough to be a fixer for a map. From this, there should also be a section on both the downloadable program and website where it showcases unverified maps (which are either AI generated maps without anyone verifying it). If the song's mapping was found from the servers database the user cannot create a new mapping of it using the AI since it already exists (only if the existing song was made / initially generated from the AI), so the user must select the existing mapping of it. To put it in another perspective (assume the AI for mapping is perfect and polished) on the webversion of this project, the AI is only allowed to create mappings of a song once, from there (if by a highly unlikely hood..) the AI messes up, the users of the program are allowed to edit and modify it to polish the mapping. But once a map is generated for the first time (meaning, the song doesn't already exist within the servers database), it becomes an unverified map. The only difference between a verified and an unverified map is if at least two registered users checked to see if the mapping of it is correct from the AI.  This is all to minimize the server's computational load. Now if the song was not found in the server's data base, then the main AI of the program maps the song into notes, when it finishes it would default to the 3d-guitar-hero like display, but can be changed to 2d version, which resembles osu!mania / stepmania layout, from a toggle switch, The website should also include the editorial of the program to modify the songs notes, whether it's because the AI tool messed up (which should be unlikely), or they want to create their own thing from scratch. The main DIFFERENCE of the software version compared to the website version is the microphone detection feature with the ranked/score mode where the user plays along to the mapping and song. Another difference of the software version compared to the website version is that it doesn't store your personal files that you've created. Another difference is maybe how there's not many intensive graphical features in the webversion if it can't run it or something. There's probably other things that I probably mentioned above but I forgot about it or they don't come to mind. 

I don't know what I have to edit really because I said a lot, but for the webversion, if user drops an audio file where the shazam-like-ai detects there to be an existing (only) verified map from the servers data base, the program uses that map file, instead of generating its own from AI. On the downloadable version of the program, if the user is connected to the internet and drops an audio file in the program where the program (again) detects there to be a verified mapping of it, the program can recommend the user to use that version if they'd like, but they can say no since the AI on the downloadable version is client-side anyway.

Maybe have chatgpt figure out these parts of the text before I finalize it and give it to chatgpt for copilot's prompt generation:

-Besides that one sentence I said I'm not too sure of, the only other thing I'm not too sure of is what's the incentive for getting karma? I'd like for you to figure out an incentive and implement it

-But I also need to find out computational costs of the combined shazam-like-ai song recognition tools plus the main AI's map generation from the song would be if I plan to have these features on the website version





-How will mobile mode be..?



just to be sure, when you use AI to generate the mapping for the drums, you first isolate the drums from the song correct? Since I feel like doing it this way, makes it such that if the user wanted to either play it back (whether its in the play mode, practice mode, or in the editorial), they can have the option to listen to either the whole song, or only the drums. Also I mentioned this before, but along with that instrumental option, I would also like you to implement a metronome option where a bpm gets played in the background, where the user can also change what sound it is in the settings of the software, the same with their skin (how the notes present) in the settings as well.



Remaining gaps to tackle: multi-stage heartbeat timing (GenerationPipeline still needs richer progress emissions), debug overlay data binding mid-run, the weighted progress helper component, backend timebase/tempo corrections, and unit tests for the new state/mapper logic.

Pipeline heartbeat cadence, weighted progress smoothing, and stage duration logging.
Debug overlay gating and data binding post-detection.
Audio/timebase corrections (tempo/downbeat detection, lane distribution, low-confidence banners).
Cancellation propagation through Demucs/Onset/Tempo/Drafting services.
Offline decode behaviour in editor preview plus tooltip.
Weighted progress bar component, info toast resource strings, unit tests for state machine/progress mapper.
Remaining logging, resilience items, and verification flow.





make sure all these things below are implemented/fixed/updated:

-GenerationPipeline.cs hands the raw AiGenerationOptions to the Python pipeline without injecting the quantised tempo/grid/offset we just computed (lines 214‚Äë244), and AiBeatmapGenerator.cs never applies a shared timebase or start-offset before serialising the beatmap (entire file). The result draft therefore still depends on the external script‚Äôs heuristics, so the ‚Äúsingle authoritative clock / downbeat anchoring / same BPM after re-run‚Äù goals aren‚Äôt met.
-The requested ‚ÄúLow detection confidence‚Äù banner and related advice path don‚Äôt exist‚Äîthere‚Äôs no string or branch for it anywhere in MappingGenerationScreen.cs, OnsetDetectionService.cs, or the pipeline.
-Instrumentation gaps remain: we don‚Äôt log note counts, selected BPM/grid, or stage durations at completion (only a generic success line); see AiBeatmapGenerator.GenerateAsync (no [gen] notes=‚Ä¶ log after load) and GenerationPipeline logs lacking BPM/grid summaries beyond a single quantize complete message.
-No unit-level tests were added. BeatSight.Tests still contains only the default UnitTest1.cs, so the ‚Äústage progress mapper‚Äù and Ready/Running UI guard scenarios aren‚Äôt covered yet.
-Wire the desktop UI to surface these override options and react to the debug metadata before moving on to low-confidence warnings.
-implement the remaining MappingGenerationScreen + pipeline changes and follow-up tests.
-smoke-test opening an offline-decode result to confirm the ‚ÄúPreview Offline‚Äù state and tooltip appear
-Decide whether to add a dedicated unit test for the new constructor signature or UI binding once more offline scenarios are covered.


Other things:

-Fix the audio/offline detection bug:
Find where we label the pipeline result as PlaybackAvailable = false and tighten that logic so we only trip the flag when BASS truly can‚Äôt open an output device (not merely because we ran an offline decode). Once fixed, the editor should keep the normal play button and avoid the ‚ÄúPreview Offline‚Äù warning anytime real audio works.
-Rebuild MappingGenerationScreen UX:
Keep the screen in an Idle state after track import, expose a Start button and advanced settings panel (sensitivity slider + quantization dropdown) that actually renders. Wire up state transitions (Idle ‚Üí Preparing ‚Üí ‚Ä¶ ‚Üí Complete/Error) so controls lock/unlock correctly and canceling returns to Idle.
-Wire heartbeat + weighted progress:
Extend the coordinator/pipeline to emit stage heartbeats and normalized progress using the provided weights. Update WeightedProgressBar to lerp smoothly on each heartbeat and show a real ‚Äústage progress %‚Äù plus a ticking ‚Äúlast update‚Äù timestamp.
-Bring the detection debug overlay to life
After onset detection, surface the analysis data (peaks, BPM, grid choice) to the screen. Add a DrawableOnsetOverlay, hook it to the existing toggle, and display a small legend summarizing counts/BPM/sensitivity.
-Confidence scoring:
Compute a quick quality score from the detection stats (peaks per minute, quantization fit error, etc.) and show the low-confidence banner only when that score drops below the threshold, with tips tailored to the underlying issue.
-Tempo disambiguation + lane distribution in the AI pipeline:
Implement the half/double BPM check and pick the tempo that minimizes quantization error. Ensure quantization respects the user-selected grid/BPM, and revisit the note assignment heuristic so kicks/snares/hats spread across lanes on drum-heavy tracks.



and more things:
-Tackle the progress/heartbeat smoothing and stage UI tweaks in MappingGenerationScreen
-Wire the detection overlay toggle and confidence banner to the new pipeline metrics.


and even more things (not sure if they're done already):
-OnsetDetectionService.cs still returns the raw detection tempo (QuantizationResult is built with baseBpm = detection.EstimatedTempo, see lines 108‚Äë149) and GenerationPipeline.cs forces that same value back into the Python run (applyAuthoritativeTiming, lines 586‚Äë613). Because options.ForceQuantization is always flipped on, the Python side (beatmap_generator.py, lines 480‚Äë520) overwrites its own half/double-time candidate search with the forced BPM/step. Net result: tempo disambiguation is not implemented and half/double errors will still occur.
-I didn‚Äôt see any legend or per-layer toggles in DetectionDebugOverlay.cs. If the checklist called for a legend/toggle UI, that work remains.
-Decide how to select the best BPM candidate (e.g. choose from QuantizationSummary.Candidates or remove ForceQuantization when confidence is low) and update applyAuthoritativeTiming accordingly.
-If a legend/toggle panel was part of the overlay acceptance criteria, extend DetectionDebugOverlay to render it.






for now:
-add cowbell to model
-find out if any of the data has chinas in it



new things:
-update the root (official) README.md to look more realistic, similar the one on ppy/osu (https://github.com/ppy/osu)
-Make the training for any of the available models and AI for this project peer available. Meaning, anyone from anywhere can add their computational power to make the models and AI stronger for this project. Preferrably put this in the program interface itself for the people who aren't knowledgable of how to go about it through a terminal. The way I see it happening is (to my knowledge at least, but I might be ignorant), when the user opens the program and creates an account (maybe optional? i'm not sure), if they wanna use their own computational power to help train and better the models and AI, they would go into the settings, enable a developer mode checkbox, that opens up a button that allows them to do this (i'm totally okay if you want to change how to go about this too). Figure out how to do this.

